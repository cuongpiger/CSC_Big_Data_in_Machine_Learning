{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit207240a302f84cf383d7b6dbf8fca3f2",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.pyspark import CPySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = CPySpark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<SparkContext master=local appName=pyspark-shell>"
      ],
      "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://192.168.43.86:4041\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.1</code></dd>\n              <dt>Master</dt>\n                <dd><code>local</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        "
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "spark.sc"
   ]
  },
  {
   "source": [
    "Tạo ra một RDD từ list of words"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_words = [\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.rdd(lst_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "type(rdd)"
   ]
  },
  {
   "source": [
    "Tạo ra một RDD từ file dữ liệu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = spark.rdd(file='./data/5000_points.txt', min_partitions=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "type(rdd2)"
   ]
  },
  {
   "source": [
    "Lấy dòng đầu tiên trong data _(giống `.head()` của pandas.DataFrame)_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'664159\\t550946'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "rdd2.first()"
   ]
  },
  {
   "source": [
    "Số vách ngăn tạo ra trong `rdd2`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "rdd2.getNumPartitions()"
   ]
  },
  {
   "source": [
    "Lấy từ URL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'hdfs://bigdata.laptrinhpython.net:19000/t8.shakespeare.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd3 = spark.rdd(file=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['This is the 100th Etext file presented by Project Gutenberg, and',\n",
       " 'is presented in cooperation with World Library, Inc., from their',\n",
       " 'Library of the Future and Shakespeare CDROMS.  Project Gutenberg',\n",
       " 'often releases Etexts that are NOT placed in the Public Domain!!',\n",
       " '',\n",
       " 'Shakespeare',\n",
       " '',\n",
       " '*This Etext has certain copyright implications you should read!*',\n",
       " '',\n",
       " '<<THIS ELECTRONIC VERSION OF THE COMPLETE WORKS OF WILLIAM']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "rdd3.take(10)"
   ]
  },
  {
   "source": [
    "<hr>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 16, 9, 25, 36, 49]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "rdd = spark.rdd([1, 4, 3, 5, 6, 7])\n",
    "rdd_map = rdd.map(lambda x: x**2)\n",
    "\n",
    "rdd_map.collect()"
   ]
  },
  {
   "source": [
    "<hr>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[4, 5, 6, 7]"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "rdd_filter = rdd.filter(lambda x: x > 3)\n",
    "numbers_all = rdd_filter.collect()\n",
    "\n",
    "numbers_all"
   ]
  },
  {
   "source": [
    "<hr>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Data', 'Science', 'Machine', 'Learning', 'Big', 'Data']"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "rdd_string = spark.rdd([\"Data Science\", \"Machine Learning\", \"Big Data\"])\n",
    "rdd_flatmap = rdd_string.flatMap(lambda x: x.split(' '))\n",
    "\n",
    "rdd_flatmap.collect()"
   ]
  },
  {
   "source": [
    "<hr>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1, 2, 4, 4, 5, 1, 3, 2, 1, 2]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "rdd1 = spark.rdd([1, 2, 4, 4, 5])\n",
    "rdd2 = spark.rdd([1, 3, 2, 1, 2])\n",
    "rdd_union = rdd1.union(rdd2)\n",
    "\n",
    "rdd_union.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "rdd_union.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "rdd_reduce = rdd_union.reduce(lambda x, y: x + y)\n",
    "\n",
    "rdd_reduce"
   ]
  },
  {
   "source": [
    "<hr>\n",
    "Lưu RDD thành file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rdd = spark.rdd(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "num_rdd.getNumPartitions()"
   ]
  },
  {
   "source": [
    "Một folder mới dc tạo ra có tên là `number` trong folder `data`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "num_rdd.saveAsTextFile('./data/number')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 23,
   "outputs": []
  },
  {
   "source": [
    "<hr>\n",
    "Gôm tất cả partitions thành một rdd duy nhất, tức ko có vách ngăn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rdd.coalesce(1).saveAsTextFile('./data/number_all')"
   ]
  },
  {
   "source": [
    "<hr>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SV001 có tổng điểm là 8\nSV002 có tổng điểm là 18\nSV003 có tổng điểm là 19\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.rdd([\n",
    "    ('SV001', 8),\n",
    "    ('SV002', 9),\n",
    "    ('SV003', 10),\n",
    "    ('SV002', 9),\n",
    "    ('SV003', 9)\n",
    "])\n",
    "\n",
    "rdd_reduce = rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "for num in rdd_reduce.collect():\n",
    "    print('{} có tổng điểm là {}'.format(num[0], num[1]))"
   ]
  },
  {
   "source": [
    "<hr>\n",
    "sort by key"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SV003 có tổng điểm là 19\nSV002 có tổng điểm là 18\nSV001 có tổng điểm là 8\n"
     ]
    }
   ],
   "source": [
    "rdd_reduce_sort = rdd_reduce.sortByKey(ascending=False)\n",
    "\n",
    "for num in rdd_reduce_sort.collect():\n",
    "    print('{} có tổng điểm là {}'.format(num[0], num[1]))"
   ]
  },
  {
   "source": [
    "<hr>\n",
    "groupByKey"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SV001 [8]\nSV002 [9, 9]\nSV003 [10, 9]\n"
     ]
    }
   ],
   "source": [
    "rdd_group = rdd.groupByKey().collect()\n",
    "\n",
    "for mssv, scores in rdd_group:\n",
    "    print(mssv, list(scores))"
   ]
  },
  {
   "source": [
    "<hr>\n",
    "join"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_math = spark.rdd([('SV001', 8), ('SV002', 9), ('SV003', 9)])\n",
    "rdd_english = spark.rdd([('SV001', 9), ('SV002', 8), ('SV003', 8)])\n",
    "\n",
    "rdd_join = rdd_math.join(rdd_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('SV002', (9, 8)), ('SV001', (8, 9)), ('SV003', (9, 8))]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "rdd_join.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_math = spark.rdd([('SV001', 8), ('SV002', 9), ('SV003', 9)])\n",
    "rdd_english = spark.rdd([('SV001', 9), ('SV002', 8)])\n",
    "\n",
    "rdd_join = rdd_math.join(rdd_english)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('SV002', (9, 8)), ('SV001', (8, 9))]"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "rdd_join.collect()"
   ]
  },
  {
   "source": [
    "<hr>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "countByKey()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "SV001 1\nSV002 2\nSV003 2\n"
     ]
    }
   ],
   "source": [
    "rdd = spark.rdd([\n",
    "    ('SV001', 8),\n",
    "    ('SV002', 9),\n",
    "    ('SV003', 10),\n",
    "    ('SV002', 9),\n",
    "    ('SV003', 9)\n",
    "])\n",
    "\n",
    "for key, val in rdd.countByKey().items():\n",
    "    print(key, val)"
   ]
  },
  {
   "source": [
    "<hr>\n",
    "collectAsMap()"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.rdd([\n",
    "    ('SV001', 8),\n",
    "    ('SV002', 9),\n",
    "    ('SV001', 10),\n",
    "    ('SV002', 6),\n",
    "    ('SV003', 9),\n",
    "    ('SV003', 8)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}