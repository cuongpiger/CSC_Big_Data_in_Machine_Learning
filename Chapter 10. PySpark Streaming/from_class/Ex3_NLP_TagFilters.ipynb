{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex3: NLP - Tags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement: Build a tags filter. Use the various NLP tools and a classifier, to predict tag for one question.  In future questions could be auto-tagged by such a classifier or tags could be recommended to users prior to posting.\n",
    "- Dataset: stack-overflow-data.csv. It contains Stack Overflow questions and associated tags.\n",
    "- Link tham khảo: http://benalexkeen.com/multiclass-text-classification-with-pyspark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '12g')\n",
    "sc = SparkContext(master='spark://172.25.51.55:7077', appName='Stack_Overflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://PM501-34:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://172.25.51.55:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Stack_Overflow</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://172.25.51.55:7077 appName=Stack_Overflow>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"hdfs://172.25.51.16:19000/stack_overflow_data.csv\"\n",
    "# file_name = \"stack-overflow-data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(file_name, inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                post|       tags|\n",
      "+--------------------+-----------+\n",
      "|what is causing t...|         c#|\n",
      "|have dynamic html...|    asp.net|\n",
      "|how to convert a ...|objective-c|\n",
      "|.net framework 4 ...|       .net|\n",
      "|trying to calcula...|     python|\n",
      "+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|         tags|count|\n",
      "+-------------+-----+\n",
      "|       iphone| 2000|\n",
      "|      android| 2000|\n",
      "|           c#| 2000|\n",
      "|         null|20798|\n",
      "|      asp.net| 2000|\n",
      "|         html| 2000|\n",
      "|        mysql| 2000|\n",
      "|       jquery| 2000|\n",
      "|   javascript| 2000|\n",
      "|          css| 2000|\n",
      "|          sql| 2000|\n",
      "|          c++| 2000|\n",
      "|            c| 2000|\n",
      "|  objective-c| 2000|\n",
      "|         java| 2000|\n",
      "|          php| 2000|\n",
      "|         .net| 2000|\n",
      "|          ios| 2000|\n",
      "|       python| 2000|\n",
      "|    angularjs| 2000|\n",
      "|ruby-on-rails| 2000|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupby('tags').count().show(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_null_data = data.filter(data.tags.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20798"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_null_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.filter(data.tags.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean and Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Create a new length feature: **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('length',length(data['post']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------+\n",
      "|                post|         tags|length|\n",
      "+--------------------+-------------+------+\n",
      "|what is causing t...|           c#|   833|\n",
      "|have dynamic html...|      asp.net|   804|\n",
      "|how to convert a ...|  objective-c|   755|\n",
      "|.net framework 4 ...|         .net|   349|\n",
      "|trying to calcula...|       python|  1290|\n",
      "|how to give alias...|      asp.net|   309|\n",
      "|window.open() ret...|    angularjs|   495|\n",
      "|identifying serve...|       iphone|   424|\n",
      "|unknown method ke...|ruby-on-rails|  2022|\n",
      "|from the include ...|    angularjs|  1279|\n",
      "|when we need inte...|           c#|   995|\n",
      "|how to install .i...|          ios|   344|\n",
      "|dynamic textbox t...|      asp.net|   389|\n",
      "|rather than bubbl...|            c|  1338|\n",
      "|site deployed in ...|      asp.net|   349|\n",
      "|connection in .ne...|         .net|   228|\n",
      "|how to subtract 1...|  objective-c|    62|\n",
      "|ror console show ...|ruby-on-rails|  2594|\n",
      "|distance between ...|       iphone|   336|\n",
      "|sql query - how t...|          sql|  1037|\n",
      "+--------------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|         tags|avg(length)|\n",
      "+-------------+-----------+\n",
      "|       iphone|    709.621|\n",
      "|      android|  1713.4345|\n",
      "|           c#|  1145.3065|\n",
      "|      asp.net|     999.95|\n",
      "|         html|   891.3105|\n",
      "|        mysql|   1038.561|\n",
      "|       jquery|   1081.507|\n",
      "|   javascript|    964.396|\n",
      "|          css|    954.809|\n",
      "|          sql|    870.912|\n",
      "|          c++|   1295.955|\n",
      "|            c|  1121.1115|\n",
      "|  objective-c|   972.8925|\n",
      "|         java|   1357.308|\n",
      "|          php|  1123.4205|\n",
      "|         .net|   731.0075|\n",
      "|          ios|   970.7565|\n",
      "|       python|  1018.6695|\n",
      "|    angularjs|  1294.7545|\n",
      "|ruby-on-rails|  1244.2055|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pretty Clear Difference\n",
    "data.groupby('tags').mean().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BsTextExtractor(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(BsTextExtractor, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "\n",
    "        def f(s):\n",
    "            cleaned_post = BeautifulSoup(s).text\n",
    "            return cleaned_post\n",
    "\n",
    "        t = StringType()\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udf(f, t)(in_col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n",
    "text_extractor = BsTextExtractor(inputCol=\"post\", outputCol=\"cleaned_post\")\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_post\", outputCol=\"token_text\")\n",
    "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
    "count_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\n",
    "idf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\n",
    "class_to_num = StringIndexer(inputCol='tags',outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model\n",
    "\n",
    "We'll use Naive Bayes, but feel free to play around with this choice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use defaults\n",
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prep_pipe = Pipeline(stages=[class_to_num,text_extractor,tokenizer,stopremove,count_vec,idf,clean_up])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = data_prep_pipe.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = cleaner.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = clean_data.select(['label','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  5.0|(262145,[0,1,2,3,...|\n",
      "|  3.0|(262145,[0,12,31,...|\n",
      "| 15.0|(262145,[0,1,2,3,...|\n",
      "|  0.0|(262145,[0,18,21,...|\n",
      "| 17.0|(262145,[0,1,4,8,...|\n",
      "|  3.0|(262145,[0,12,21,...|\n",
      "|  2.0|(262145,[0,1,3,6,...|\n",
      "| 10.0|(262145,[0,44,61,...|\n",
      "| 18.0|(262145,[0,1,14,2...|\n",
      "|  2.0|(262145,[0,1,3,4,...|\n",
      "|  5.0|(262145,[0,2,3,6,...|\n",
      "|  9.0|(262145,[0,18,27,...|\n",
      "|  3.0|(262145,[0,7,12,1...|\n",
      "|  4.0|(262145,[0,1,2,3,...|\n",
      "|  3.0|(262145,[0,11,27,...|\n",
      "|  0.0|(262145,[0,187,23...|\n",
      "| 15.0|(262145,[0,10,15,...|\n",
      "| 18.0|(262145,[0,1,3,12...|\n",
      "| 10.0|(262145,[0,30,39,...|\n",
      "| 19.0|(262145,[0,12,15,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_data.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training,testing) = clean_data.randomSplit([0.7,0.3], seed=142)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = nb.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = predictor.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(262145,[0,1,2,3,...|[-9525.9204663975...|[2.06145944879209...|       5.0|\n",
      "|  0.0|(262145,[0,1,2,3,...|[-11222.607664018...|[1.09156029556411...|       3.0|\n",
      "|  0.0|(262145,[0,1,2,3,...|[-5699.5010733911...|[1.27980939167733...|       5.0|\n",
      "|  0.0|(262145,[0,1,2,3,...|[-8624.8868314121...|[6.68896536535949...|       5.0|\n",
      "|  0.0|(262145,[0,1,2,3,...|[-3034.0894816783...|[0.99999999417313...|       0.0|\n",
      "|  0.0|(262145,[0,1,2,3,...|[-3393.1534857104...|[1.65827521693478...|       3.0|\n",
      "|  0.0|(262145,[0,1,7,8,...|[-6028.1757418936...|[1.0,0.0,0.0,2.19...|       0.0|\n",
      "|  0.0|(262145,[0,1,8,9,...|[-2239.4267154832...|[7.37356324710894...|       3.0|\n",
      "|  0.0|(262145,[0,1,8,11...|[-2779.4219594575...|[3.54937613564798...|       3.0|\n",
      "|  0.0|(262145,[0,1,10,1...|[-2558.9198631067...|[0.99994927017905...|       0.0|\n",
      "|  0.0|(262145,[0,1,11,1...|[-14049.371470545...|[3.56451465508441...|       3.0|\n",
      "|  0.0|(262145,[0,1,11,1...|[-2073.9189441840...|[9.68380921419492...|      15.0|\n",
      "|  0.0|(262145,[0,2,3,7,...|[-4756.7721929863...|[6.7418806501626E...|       5.0|\n",
      "|  0.0|(262145,[0,2,3,9,...|[-1231.3215526546...|[9.78781592530668...|      11.0|\n",
      "|  0.0|(262145,[0,2,3,13...|[-4603.0179871720...|[6.87149243619445...|       5.0|\n",
      "|  0.0|(262145,[0,2,3,28...|[-2109.0851194540...|[1.45680306248163...|       5.0|\n",
      "|  0.0|(262145,[0,4,21,4...|[-4110.5292409029...|[1.59845126429504...|      19.0|\n",
      "|  0.0|(262145,[0,7,8,22...|[-5900.2088944552...|[5.66575377058671...|       3.0|\n",
      "|  0.0|(262145,[0,7,9,12...|[-1777.3157394126...|[1.0,4.3976515984...|       0.0|\n",
      "|  0.0|(262145,[0,7,11,1...|[-3224.6713040043...|[2.57867176415578...|       3.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  8.0|       3.0|   25|\n",
      "| 12.0|      16.0|   15|\n",
      "| 16.0|       8.0|   24|\n",
      "| 19.0|       5.0|    8|\n",
      "| 10.0|       1.0|    5|\n",
      "|  2.0|       0.0|    3|\n",
      "| 15.0|      16.0|    2|\n",
      "| 12.0|       5.0|   14|\n",
      "|  0.0|      12.0|    7|\n",
      "|  1.0|      19.0|    3|\n",
      "|  0.0|       8.0|    5|\n",
      "|  1.0|      12.0|    1|\n",
      "| 19.0|      12.0|    2|\n",
      "|  7.0|       3.0|    3|\n",
      "| 15.0|      11.0|    6|\n",
      "| 11.0|      17.0|    7|\n",
      "| 17.0|      19.0|    8|\n",
      "|  6.0|       1.0|    1|\n",
      "|  8.0|       6.0|    1|\n",
      "| 16.0|      10.0|    1|\n",
      "+-----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "test_results.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.7175324986751529\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc = acc_eval.evaluate(test_results)\n",
    "print(\"Accuracy of model at predicting: {}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save may cuc bo\n",
    "# nb.save(\"NB_TagFilters_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save hdfs\n",
    "nb.save(\"hdfs://172.25.51.16:19000/NB_TagFilters_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not very good result! (~72%)\n",
    "- Solution: Try switching out the classification models! Or even try to come up with other engineered features!..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use LogisticRegression/Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = LogisticRegression(maxIter=100, regParam=0.3, elasticNetParam=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_1 = lg.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_1 = predictor_1.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  8.0|       3.0|   13|\n",
      "| 12.0|      16.0|   13|\n",
      "| 16.0|       8.0|   80|\n",
      "| 19.0|       5.0|    6|\n",
      "| 10.0|       1.0|    6|\n",
      "|  2.0|       0.0|    2|\n",
      "| 15.0|      16.0|    1|\n",
      "| 12.0|       5.0|    9|\n",
      "|  0.0|      12.0|    4|\n",
      "|  0.0|       8.0|   26|\n",
      "|  1.0|      19.0|    3|\n",
      "|  1.0|      12.0|    3|\n",
      "|  7.0|       3.0|    2|\n",
      "| 15.0|      11.0|    4|\n",
      "| 19.0|      12.0|    2|\n",
      "| 11.0|      17.0|    8|\n",
      "| 17.0|      19.0|   15|\n",
      "| 17.0|       7.0|    2|\n",
      "| 16.0|      10.0|   20|\n",
      "|  6.0|       1.0|    3|\n",
      "+-----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "test_results_1.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.6988642446771226\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_1 = acc_eval.evaluate(test_results_1)\n",
    "print(\"Accuracy of model at predicting: {}\".format(acc_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It's not better result!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save máy cục bộ\n",
    "# lg.save(\"LG_TagFilters_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save HDFS\n",
    "lg.save(\"hdfs://172.25.51.16:19000/LG_TagFilters_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"label\", \\\n",
    "                            featuresCol=\"features\", \\\n",
    "                            numTrees = 500, \\\n",
    "                            maxDepth = 5, \\\n",
    "                            maxBins = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_2 = rf.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_2 = predictor_2.transform(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  8.0|       3.0|   11|\n",
      "| 12.0|      16.0|    7|\n",
      "| 16.0|       8.0|   22|\n",
      "| 19.0|       5.0|    1|\n",
      "| 10.0|       1.0|    7|\n",
      "| 15.0|      16.0|    1|\n",
      "| 12.0|       5.0|    6|\n",
      "|  1.0|      19.0|    8|\n",
      "|  0.0|       8.0|    6|\n",
      "|  0.0|      12.0|    3|\n",
      "| 15.0|      11.0|    3|\n",
      "| 17.0|       7.0|    1|\n",
      "| 11.0|      17.0|   10|\n",
      "| 17.0|      19.0|    9|\n",
      "| 16.0|      10.0|   26|\n",
      "|  8.0|       6.0|    1|\n",
      "|  4.0|       6.0|   19|\n",
      "|  3.0|       5.0|   39|\n",
      "|  9.0|       5.0|    3|\n",
      "| 15.0|      19.0|    9|\n",
      "+-----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a confusion matrix\n",
    "test_results_2.groupBy('label', 'prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|       8.0|  584|\n",
      "|       0.0|  414|\n",
      "|       7.0|  659|\n",
      "|      18.0|  601|\n",
      "|       1.0|  532|\n",
      "|       4.0|  614|\n",
      "|      11.0|  571|\n",
      "|      14.0|  545|\n",
      "|      19.0|  819|\n",
      "|       3.0|  452|\n",
      "|       2.0|  583|\n",
      "|      17.0|  640|\n",
      "|      10.0| 1442|\n",
      "|      13.0|  624|\n",
      "|       6.0|  510|\n",
      "|       5.0|  481|\n",
      "|      15.0|  587|\n",
      "|       9.0|  341|\n",
      "|      16.0|  524|\n",
      "|      12.0|  398|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_results_2.groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model at predicting: 0.7502639717792994\n"
     ]
    }
   ],
   "source": [
    "acc_eval = MulticlassClassificationEvaluator()\n",
    "acc_2 = acc_eval.evaluate(test_results_2)\n",
    "print(\"Accuracy of model at predicting: {}\".format(acc_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## It has higher accuracy but is not a better result!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save máy cục bộ\n",
    "# rf.save(\"RF_TagFilters_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save HDFS\n",
    "rf.save(\"hdfs://172.25.51.16:19000/RF_TagFilters_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
