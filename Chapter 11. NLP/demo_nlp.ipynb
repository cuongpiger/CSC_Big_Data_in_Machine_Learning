{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit739b5682ab17410ead4710a41bd80672",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from modules.my_pyspark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = MyPySpark(session=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentenceDataFrame = spark.session.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark\"),\n",
    "    (1, \"I know Spark can work well with NLP\"),\n",
    "    (2, \"Logistic,regression,models,are,supervised\")\n",
    "], ['id', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----------------------------------------+\n|id |sentence                                 |\n+---+-----------------------------------------+\n|0  |Hi I heard about Spark                   |\n|1  |I know Spark can work well with NLP      |\n|2  |Logistic,regression,models,are,supervised|\n+---+-----------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='sentence', outputCol='words')\n",
    "regexTokenizer = RegexTokenizer(inputCol='sentence', outputCol='words', pattern=\"\\\\W\")\n",
    "                                                                                # alternative: pattern=\"\\\\w+\", gaps(False)\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------------------------+--------------------------------------------+------+\n|sentence                                 |words                                       |tokens|\n+-----------------------------------------+--------------------------------------------+------+\n|Hi I heard about Spark                   |[hi, i, heard, about, spark]                |5     |\n|I know Spark can work well with NLP      |[i, know, spark, can, work, well, with, nlp]|8     |\n|Logistic,regression,models,are,supervised|[logistic,regression,models,are,supervised] |1     |\n+-----------------------------------------+--------------------------------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "tokenized.select('sentence', 'words').withColumn('tokens', countTokens(col('words'))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "regexTokenized = regexTokenizer.transform(sentenceDataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------------------------+-----------------------------------------------+------+\n|sentence                                 |words                                          |tokens|\n+-----------------------------------------+-----------------------------------------------+------+\n|Hi I heard about Spark                   |[hi, i, heard, about, spark]                   |5     |\n|I know Spark can work well with NLP      |[i, know, spark, can, work, well, with, nlp]   |8     |\n|Logistic,regression,models,are,supervised|[logistic, regression, models, are, supervised]|5     |\n+-----------------------------------------+-----------------------------------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "regexTokenized.select('sentence', 'words').withColumn('tokens', countTokens(col('words'))).show(truncate=False)"
   ]
  },
  {
   "source": [
    "# Stopword remover"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = regexTokenized.withColumn('tokens', countTokens(col('words')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+--------------------+--------------------+------+\n| id|            sentence|               words|tokens|\n+---+--------------------+--------------------+------+\n|  0|Hi I heard about ...|[hi, i, heard, ab...|     5|\n|  1|I know Spark can ...|[i, know, spark, ...|     8|\n|  2|Logistic,regressi...|[logistic, regres...|     5|\n+---+--------------------+--------------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "x.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---+-----------------------------------------+-----------------------------------------------+------+------------------------------------------+\n|id |sentence                                 |words                                          |tokens|filtered                                  |\n+---+-----------------------------------------+-----------------------------------------------+------+------------------------------------------+\n|0  |Hi I heard about Spark                   |[hi, i, heard, about, spark]                   |5     |[hi, heard, spark]                        |\n|1  |I know Spark can work well with NLP      |[i, know, spark, can, work, well, with, nlp]   |8     |[know, spark, work, well, nlp]            |\n|2  |Logistic,regression,models,are,supervised|[logistic, regression, models, are, supervised]|5     |[logistic, regression, models, supervised]|\n+---+-----------------------------------------+-----------------------------------------------+------+------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol='words', outputCol='filtered')\n",
    "remover.transform(x).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover_df = remover.transform(x)"
   ]
  },
  {
   "source": [
    "# NGram"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------------------------------------------+\n|ngrams                                                     |\n+-----------------------------------------------------------+\n|[hi heard, heard spark]                                    |\n|[know spark, spark work, work well, well nlp]              |\n|[logistic regression, regression models, models supervised]|\n+-----------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "ngram = NGram(n=2, inputCol='filtered', outputCol='ngrams')\n",
    "ngramDataFrame = ngram.transform(remover_df)\n",
    "ngramDataFrame.select('ngrams').show(truncate=False)"
   ]
  },
  {
   "source": [
    "# TF-IDF"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsData = ngramDataFrame = ngram.transform(remover_df).select('ngrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------------------------------------------+--------------------------+\n|ngrams                                                     |rawFeatures               |\n+-----------------------------------------------------------+--------------------------+\n|[hi heard, heard spark]                                    |(10,[1],[2.0])            |\n|[know spark, spark work, work well, well nlp]              |(10,[4,6,7],[1.0,2.0,1.0])|\n|[logistic regression, regression models, models supervised]|(10,[0,1,3],[1.0,1.0,1.0])|\n+-----------------------------------------------------------+--------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "hashingTF = HashingTF(inputCol='ngrams', outputCol='rawFeatures', numFeatures=10)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "featurizedData.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------------------------------------------------------------+\n|features                                                                |\n+------------------------------------------------------------------------+\n|(10,[1],[0.5753641449035617])                                           |\n|(10,[4,6,7],[0.6931471805599453,1.3862943611198906,0.6931471805599453]) |\n|(10,[0,1,3],[0.6931471805599453,0.28768207245178085,0.6931471805599453])|\n+------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "rescaledData.select('features').show(truncate=False)"
   ]
  },
  {
   "source": [
    "# CountVectorizer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.session.createDataFrame([\n",
    "    (0, \"a b c\".split(' ')),\n",
    "    (1, \"a b b c a\".split(\" \")),\n",
    "    (2, \"a b d d a c c\".split(\" \"))\n",
    "], ['id', 'words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------------------------------------------------+-------------------+\n|ngrams                                                     |features_cv        |\n+-----------------------------------------------------------+-------------------+\n|[hi heard, heard spark]                                    |(4,[3],[1.0])      |\n|[know spark, spark work, work well, well nlp]              |(4,[0,2],[1.0,1.0])|\n|[logistic regression, regression models, models supervised]|(4,[1],[1.0])      |\n+-----------------------------------------------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(inputCol='ngrams', outputCol='features_cv', vocabSize=4, minDF=1)\n",
    "model =cv.fit(wordsData)\n",
    "res = model.transform(wordsData)\n",
    "res.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}