{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bit739b5682ab17410ead4710a41bd80672",
   "display_name": "Python 3.8.5 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "\n",
    "for i in range(1, 6):\n",
    "    sheet = \"Sheet\" + str(i)\n",
    "    df = pd.read_excel(\"../data/CCPP/Folds5x2_pp.xlsx\", sheet_name=sheet)\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfs[0]\n",
    "\n",
    "for i in range(1, 5):\n",
    "    df = df.append(dfs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         AT      V       AP     RH      PE\n",
       "0     14.96  41.76  1024.07  73.17  463.26\n",
       "1     25.18  62.96  1020.04  59.08  444.37\n",
       "2      5.11  39.40  1012.16  92.14  488.56\n",
       "3     20.86  57.32  1010.24  76.64  446.48\n",
       "4     10.82  37.50  1009.23  96.62  473.90\n",
       "...     ...    ...      ...    ...     ...\n",
       "9563  15.12  48.92  1011.80  72.93  462.59\n",
       "9564  33.41  77.95  1010.30  59.72  432.90\n",
       "9565  15.99  43.34  1014.20  78.66  465.96\n",
       "9566  17.65  59.87  1018.58  94.65  450.93\n",
       "9567  23.68  51.30  1011.86  71.24  451.67\n",
       "\n",
       "[47840 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AT</th>\n      <th>V</th>\n      <th>AP</th>\n      <th>RH</th>\n      <th>PE</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>14.96</td>\n      <td>41.76</td>\n      <td>1024.07</td>\n      <td>73.17</td>\n      <td>463.26</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25.18</td>\n      <td>62.96</td>\n      <td>1020.04</td>\n      <td>59.08</td>\n      <td>444.37</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.11</td>\n      <td>39.40</td>\n      <td>1012.16</td>\n      <td>92.14</td>\n      <td>488.56</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>20.86</td>\n      <td>57.32</td>\n      <td>1010.24</td>\n      <td>76.64</td>\n      <td>446.48</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10.82</td>\n      <td>37.50</td>\n      <td>1009.23</td>\n      <td>96.62</td>\n      <td>473.90</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9563</th>\n      <td>15.12</td>\n      <td>48.92</td>\n      <td>1011.80</td>\n      <td>72.93</td>\n      <td>462.59</td>\n    </tr>\n    <tr>\n      <th>9564</th>\n      <td>33.41</td>\n      <td>77.95</td>\n      <td>1010.30</td>\n      <td>59.72</td>\n      <td>432.90</td>\n    </tr>\n    <tr>\n      <th>9565</th>\n      <td>15.99</td>\n      <td>43.34</td>\n      <td>1014.20</td>\n      <td>78.66</td>\n      <td>465.96</td>\n    </tr>\n    <tr>\n      <th>9566</th>\n      <td>17.65</td>\n      <td>59.87</td>\n      <td>1018.58</td>\n      <td>94.65</td>\n      <td>450.93</td>\n    </tr>\n    <tr>\n      <th>9567</th>\n      <td>23.68</td>\n      <td>51.30</td>\n      <td>1011.86</td>\n      <td>71.24</td>\n      <td>451.67</td>\n    </tr>\n  </tbody>\n</table>\n<p>47840 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['AT', 'V', 'AP', 'RH', 'PE'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.utils import createSparkDfFromXlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = createSparkDfFromXlsx(df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----+-----+-------+-----+------+\n|   AT|    V|     AP|   RH|    PE|\n+-----+-----+-------+-----+------+\n|14.96|41.76|1024.07|73.17|463.26|\n|25.18|62.96|1020.04|59.08|444.37|\n| 5.11| 39.4|1012.16|92.14|488.56|\n|20.86|57.32|1010.24|76.64|446.48|\n|10.82| 37.5|1009.23|96.62| 473.9|\n+-----+-----+-------+-----+------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "47840"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9527"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "9527"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "root\n |-- AT: double (nullable = true)\n |-- V: double (nullable = true)\n |-- AP: double (nullable = true)\n |-- RH: double (nullable = true)\n |-- PE: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "source": [
    "# Chuyển dữ liệu"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['AT', 'V', 'AP', 'RH'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = assembler.transform(data).select('features', 'PE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+------+\n|            features|    PE|\n+--------------------+------+\n|[24.54,60.29,1017...|447.67|\n|[10.59,42.49,1009...|477.49|\n|[26.7,66.56,1005....|430.21|\n+--------------------+------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "data.show(3)"
   ]
  },
  {
   "source": [
    "# Scale"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol='features', outputCol='scaled_features', withStd=False, withMean=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_model = scaler.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_data = scaler_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_data = scale_data.select('scaled_features', 'PE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+------+\n|     scaled_features|    PE|\n+--------------------+------+\n|[4.88177495538994...|447.67|\n|[-9.0682250446100...|477.49|\n|[7.04177495538994...|430.21|\n+--------------------+------+\nonly showing top 3 rows\n\n"
     ]
    }
   ],
   "source": [
    "scale_data.show(3)"
   ]
  },
  {
   "source": [
    "# Tính correlation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr = Correlation.corr(data, 'features').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DenseMatrix([[ 1.        ,  0.84368857, -0.50822164, -0.54394686],\n             [ 0.84368857,  1.        , -0.41571837, -0.31221399],\n             [-0.50822164, -0.41571837,  1.        ,  0.10163098],\n             [-0.54394686, -0.31221399,  0.10163098,  1.        ]])\n"
     ]
    }
   ],
   "source": [
    "print(str(pearson_corr).replace('nan', 'NaN'))"
   ]
  },
  {
   "source": [
    "> * AT và V có tương quan cao"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Tach dữ liệu thành train và test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+------+\n|            features| label|\n+--------------------+------+\n|[24.54,60.29,1017...|447.67|\n|[10.59,42.49,1009...|477.49|\n|[26.7,66.56,1005....|430.21|\n|[21.24,41.67,1012...|459.81|\n|[27.74,74.78,1010...|436.87|\n|[9.08,36.71,1025....|479.02|\n|[25.06,65.46,1014...|443.03|\n|[30.2,73.67,1006....|428.72|\n|[16.53,46.18,1010...|458.67|\n|[22.55,70.79,1006...|436.43|\n|[17.66,60.08,1017...|456.62|\n|[23.17,62.39,1008...|439.29|\n|[23.89,65.18,1012...|438.97|\n|[26.2,65.18,1011....|440.07|\n|[26.01,74.33,1015...|435.82|\n|[28.26,72.43,1006...| 426.2|\n|[15.37,43.34,1014...|460.02|\n|[15.4,38.73,1000....|469.18|\n|[28.15,72.99,1007...|431.83|\n|[23.78,49.3,1003....|439.83|\n+--------------------+------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "data = data.withColumnRenamed('PE', 'label')\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = data.randomSplit((0.8, 0.2))"
   ]
  },
  {
   "source": [
    "# Build model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preditions_0 = lr_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+------+------------------+\n|            features| label|        prediction|\n+--------------------+------+------------------+\n|[6.71,40.96,1022....|486.58| 480.7724716363177|\n|[15.73,38.73,1002...|466.63|464.61261024512356|\n|[30.95,73.06,1008...|431.77|427.08816241057093|\n|[5.04,40.64,1021....|484.42|  483.802404961832|\n|[6.57,42.07,1004....|483.86|482.43962391931274|\n+--------------------+------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "preditions_0.show(5)"
   ]
  },
  {
   "source": [
    "# Đánh giá"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prediction = lr_model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9238525403528323"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "model_prediction.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "21.950333659881906"
      ]
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "model_prediction.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "4.685118318664098"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "model_prediction.rootMeanSquaredError"
   ]
  },
  {
   "source": [
    "> **Nhận xét**\n",
    "> * Kết quả rất tốt"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}